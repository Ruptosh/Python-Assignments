{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 19 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30. ?\n",
    "1. Using the k-means method, create two clusters for each set of centroid described above.\n",
    "2. For each set of centroid values, calculate the SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f8caa",
   "metadata": {},
   "source": [
    "**Ans:-** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. Describe how the Market Basket Research makes use of association analysis concepts ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee597f45",
   "metadata": {},
   "source": [
    "**Ans:-** Market Basket Analysis (MBA) is a data mining technique that involves analyzing transactions and discovering associations between products based on customers' purchasing patterns. The primary goal of MBA is to identify the relationships between products that customers are likely to purchase together. The analysis is performed on transactional data, which contains information about customer purchases, such as the date and time of the purchase and the products purchased.\n",
    "\n",
    "Association analysis concepts are used in MBA to discover patterns and relationships between products. These concepts are used to identify items that are frequently purchased together, known as itemsets. In MBA, itemsets are used to generate association rules, which describe the probability of purchasing one item given the purchase of another item.\n",
    "\n",
    "Association rules are generated using two key measures: support and confidence. Support measures the frequency of an itemset in the transactional data, while confidence measures the probability that an itemset B will be purchased given the purchase of itemset A. The Apriori algorithm is commonly used in MBA to generate frequent itemsets and association rules.\n",
    "\n",
    "MBA is used in various industries, including retail, e-commerce, and marketing, to drive sales and customer loyalty. For example, retailers can use MBA to identify the products that customers frequently purchase together and use this information to design promotions, discounts, or product recommendations that incentivize customers to purchase these products together. In addition, MBA can be used to identify cross-selling opportunities and optimize product placement within stores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. Give an example of the Apriori algorithm for learning association rules ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8c62e",
   "metadata": {},
   "source": [
    "**Ans:-** Suppose we have a dataset of transactions from a grocery store, and we want to use the Apriori algorithm to identify frequent itemsets and association rules.\n",
    "\n",
    "Suppose the dataset looks like this:\n",
    "\n",
    "Transaction 1: {bread, milk, eggs}\n",
    "Transaction 2: {bread, milk, cheese}\n",
    "Transaction 3: {milk, cheese}\n",
    "Transaction 4: {bread, butter}\n",
    "Transaction 5: {bread, milk, cheese, butter}\n",
    "Transaction 6: {bread, milk, cheese, butter, eggs}\n",
    "Transaction 7: {bread, milk, cheese, butter, eggs, yogurt}\n",
    "Transaction 8: {milk, eggs}\n",
    "Transaction 9: {bread, milk, butter}\n",
    "Transaction 10: {bread, milk, cheese, butter, yogurt}\n",
    "Suppose we want to find all itemsets that occur in at least 3 transactions. We can use the following steps:\n",
    "\n",
    "First, we count the occurrences of individual items. We find that bread occurs in 7 transactions, milk occurs in 8 transactions, eggs occur in 3 transactions, cheese occurs in 5 transactions, and so on.\n",
    "\n",
    "We then generate all possible pairs of items, and count the occurrences of each pair. We find that {bread, milk} occurs in 6 transactions, {bread, eggs} occurs in 2 transactions, {bread, cheese} occurs in 3 transactions, and so on.\n",
    "\n",
    "We continue this process for triples, quadruples, and so on, until we have found all frequent itemsets.\n",
    "\n",
    "Finally, we can generate association rules from the frequent itemsets. For example, we might find the following rule: {bread, milk} -> {cheese}, with support 3/10 and confidence 3/6.\n",
    "\n",
    "The Apriori algorithm is used to efficiently find frequent itemsets and association rules in large datasets, by using the principle of downward closure and avoiding the need to generate all possible itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f0005",
   "metadata": {},
   "source": [
    "**Ans:-** In hierarchical clustering, the distance between clusters is measured by a distance metric, which determines the distance between two clusters based on the distances between their individual data points. There are several distance metrics that can be used, such as Euclidean distance, Manhattan distance, and cosine distance, among others.\n",
    "\n",
    "Once the distance between clusters is calculated, the algorithm proceeds by iteratively merging the two closest clusters into a larger one until all data points belong to the same cluster. The distance between clusters is used as a stopping criterion for the algorithm. Typically, a stopping distance is defined beforehand, and the algorithm is halted when the distance between the two closest clusters exceeds this threshold. This is because when the distance between clusters is large, it indicates that the two clusters are not very similar and should not be merged. Conversely, when the distance between clusters is small, it indicates that the two clusters are very similar and should be merged.\n",
    "\n",
    "The stopping distance is chosen based on the specific problem and the desired level of clustering. A smaller stopping distance will result in more clusters, while a larger stopping distance will result in fewer clusters. The choice of stopping distance also depends on the distance metric used, as some distance metrics may produce larger or smaller distances between clusters for the same level of similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. In the k-means algorithm, how do you recompute the cluster centroids ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c1ca6",
   "metadata": {},
   "source": [
    "**Ans:-** In the k-means algorithm, after assigning each data point to a cluster, the cluster centroid needs to be updated based on the mean of all the points in that cluster. This is done by computing the mean vector of each cluster, which represents the new location of the centroid. The mean vector is calculated by averaging the values of each feature across all the points in the cluster. The formula for the new centroid of cluster k is:\n",
    "\n",
    "new_centroid_k = (1/n_k) * sum(x_i) for all x_i in cluster k\n",
    "\n",
    "Where n_k is the number of points in cluster k, and x_i is the vector representing the i-th data point. This computation is done for each cluster, and the resulting mean vectors are used as the new centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. At the start of the clustering exercise, discuss one method for determining the required number of clusters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840598f",
   "metadata": {},
   "source": [
    "**Ans:-** There are several methods to determine the required number of clusters for a clustering exercise. One such method is the elbow method. The elbow method involves plotting the number of clusters against the sum of squared distances between data points and their corresponding cluster centroids (also known as the inertia). As the number of clusters increases, the inertia typically decreases, as each data point is assigned to a closer centroid. However, beyond a certain number of clusters, the decrease in inertia slows down, resulting in a plot that looks like an arm with an elbow. The \"elbow\" point on the plot indicates the number of clusters beyond which the decrease in inertia is not significant. This number can be chosen as the required number of clusters for the clustering exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. Discuss the k-means algorithm's advantages and disadvantages ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee165882",
   "metadata": {},
   "source": [
    "**Ans:-** The k-means algorithm is a popular clustering algorithm that has several advantages and disadvantages.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simplicity: The k-means algorithm is relatively simple to implement and understand, making it a popular choice for clustering tasks.\n",
    "\n",
    "Scalability: The algorithm can handle large datasets with a large number of features.\n",
    "\n",
    "Efficiency: The k-means algorithm is computationally efficient and can quickly converge to a local minimum.\n",
    "\n",
    "Versatility: The k-means algorithm can be applied to a variety of clustering problems, including image segmentation, document clustering, and customer segmentation.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Sensitivity to initial centroids: The k-means algorithm is sensitive to the initial placement of the cluster centroids, and different initial centroids can result in different final clustering results.\n",
    "\n",
    "Requires pre-specification of k: The number of clusters, k, needs to be specified before running the algorithm. If the number of clusters is not known, it can be difficult to determine the optimal value of k.\n",
    "\n",
    "Limited to Euclidean distance: The k-means algorithm is limited to Euclidean distance, which can lead to suboptimal clustering results for non-Euclidean data.\n",
    "\n",
    "Prone to local optima: The k-means algorithm can converge to a local minimum instead of a global minimum, leading to suboptimal clustering results.\n",
    "\n",
    "Overall, the k-means algorithm is a popular clustering algorithm that is relatively easy to implement and can handle large datasets. However, it has some limitations, such as sensitivity to initial centroids and the need to specify the number of clusters before running the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. Draw a diagram to demonstrate the principle of clustering ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707d936",
   "metadata": {},
   "source": [
    "**Ans:-** Clustering is a machine learning technique used to group data points together based on their similarities. The aim is to find patterns and structures within the data that can help to better understand it or to make predictions about new data.\n",
    "\n",
    "The process of clustering can be imagined as separating a bag of marbles into smaller bags of marbles with similar colors. The marbles represent the data points, and the colors represent the features or attributes of the data points. In clustering, the algorithm separates the data points into groups based on their similarities, just as you would separate the marbles by color. The result is a set of clusters that contain data points that are similar to each other, but different from the data points in other clusters.\n",
    "\n",
    "The diagram of the clustering process would show a bag of marbles at the top, with arrows pointing to smaller bags of marbles below. Each smaller bag would represent a cluster of data points that have been grouped together based on their similarities. The marbles within each bag would have similar colors, representing the features or attributes of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "##### 9. During your study, you discovered seven findings, which are listed in the data points below. Using the K-means algorithm, you want to build three clusters from these observations. The clusters C1, C2, and C3 have the following findings after the first iteration ?\n",
    "- `C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),  `\n",
    "- `C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,  `\n",
    "- `C3: (5,5) and (9,9) ` \n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this clustering's SSE be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab74816",
   "metadata": {},
   "source": [
    "**Ans:-** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "##### 10. In a software project, the team is attempting to determine if software flaws discovered during testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters of related defects. Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. A simple diagram can be used to explain this process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b6502",
   "metadata": {},
   "source": [
    "**Ans:-** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
